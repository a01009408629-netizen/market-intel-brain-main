# Advanced Telemetry System

A comprehensive observability system using OpenTelemetry for distributed tracing and Prometheus for metrics collection, providing complete instrumentation for adapter monitoring and Grafana visualization.

## ğŸš€ **Core Features**

### **ğŸ“Š OpenTelemetry Distributed Tracing**
- **Span Creation**: Automatic span creation for each adapter operation
- **Distributed Context**: Trace context propagation across services
- **Jaeger Integration**: Export traces to Jaeger for visualization
- **Performance Monitoring**: Automatic execution time tracking
- **Error Tracking**: Comprehensive error recording and analysis

### **ğŸ“ˆ Prometheus Metrics Collection**
- **Latency Histograms**: Track request latency distributions
- **Success/Failure Counters**: Monitor provider reliability
- **Active Requests Gauges**: Real-time request monitoring
- **Custom Metrics**: Flexible metric creation for specific needs
- **Data Volume Tracking**: Monitor data processing volumes

### **ğŸ”§ Automatic Instrumentation**
- **Middleware Pattern**: Automatic adapter class instrumentation
- **Decorator Support**: Easy function-level telemetry
- **Zero Business Logic Impact**: No changes to core logic required
- **Flexible Configuration**: Configurable sampling and filtering

### **ğŸ“¡ Data Collection & Aggregation**
- **Centralized Collector**: Unified data collection from all sources
- **Real-time Processing**: Background event processing and aggregation
- **Time-window Aggregations**: Sliding window statistics
- **Event Filtering**: Configurable event type and provider filtering
- **Persistence Support**: Optional event persistence for analysis

## ğŸ“ **Structure**

```
telemetry/
â”œâ”€â”€ __init__.py              # Main exports and global instances
â”œâ”€â”€ exceptions.py            # Custom telemetry exceptions
â”œâ”€â”€ tracer.py              # OpenTelemetry tracer implementation
â”œâ”€â”€ metrics.py              # Prometheus metrics collection
â”œâ”€â”€ middleware.py           # Automatic instrumentation middleware
â”œâ”€â”€ collector.py            # Centralized data collector
â”œâ”€â”€ exporter.py             # Prometheus HTTP exporter
â”œâ”€â”€ example_usage.py        # Comprehensive examples
â”œâ”€â”€ requirements.txt        # Dependencies
â””â”€â”€ README.md             # This file
```

## ğŸ”§ **Installation**

```bash
pip install -r requirements.txt
```

## ğŸ’¡ **Quick Start**

### **Basic Tracing**

```python
from telemetry import get_tracer, trace_span

# Get global tracer
tracer = get_tracer(service_name="my-app")

# Create spans manually
with tracer.trace_async("operation_name") as span:
    span.set_attribute("user_id", "user123")
    # Your code here
    result = await some_operation()
    span.set_attribute("result_count", len(result))
```

### **Automatic Adapter Instrumentation**

```python
from telemetry import instrument_adapter

# Instrument adapter class
@instrument_adapter("my_provider")
class MyAdapter:
    def fetch_data(self, symbol):
        # Automatically traced!
        return api_call(symbol)
```

### **Prometheus Metrics**

```python
from telemetry import get_metrics

# Get global metrics
metrics = get_metrics()

# Record request success
metrics.record_request_success(
    provider="my_provider",
    operation="fetch_data",
    duration=0.5
)

# Record error
metrics.record_request_error(
    provider="my_provider",
    operation="fetch_data",
    error_type="timeout"
)
```

### **Telemetry Decorator**

```python
from telemetry import telemetry_decorator

@telemetry_decorator(provider_name="my_provider")
async def expensive_operation(data):
    # Automatically traced and measured!
    result = await process_data(data)
    return result
```

## ğŸ—ï¸ **Architecture Overview**

### **Tracing Architecture**

```python
# Span creation with automatic context
with tracer.trace_async("adapter.fetch") as span:
    span.set_attribute("provider", "finnhub")
    span.set_attribute("symbol", "AAPL")
    
    # Add events to span
    tracer.add_span_event(span, "cache_hit", {"cache_key": "AAPL"})
    
    # Your code here
    result = await fetch_from_provider("AAPL")
    return result
```

### **Metrics Architecture**

```python
# Request metrics (automatically updated)
metrics.request_total.labels(
    provider="finnhub",
    operation="fetch",
    status="success"
).inc()

# Latency histogram (automatically observed)
metrics.request_duration.labels(
    provider="finnhub",
    operation="fetch"
).observe(duration)

# Active requests gauge
metrics.active_requests.labels(provider="finnhub").inc()
# ... request completes
metrics.active_requests.labels(provider="finnhub").dec()
```

### **Instrumentation Middleware**

```python
# Automatic class instrumentation
middleware = TelemetryMiddleware()

# Instrument entire class
instrumented_adapter = middleware.instrument_class(MyAdapter)

# Use instrumented adapter (telemetry is automatic)
adapter = instrumented_adapter()
result = await adapter.fetch_data("AAPL")
```

## ğŸ¯ **Advanced Usage**

### **Custom Metrics Creation**

```python
from telemetry import get_metrics

metrics = get_metrics()

# Custom counter
custom_counter = metrics.create_custom_counter(
    "custom_operations_total",
    "Total custom operations",
    ["operation_type", "status"]
)

# Custom histogram
custom_histogram = metrics.create_custom_histogram(
    "custom_operation_duration_seconds",
    "Custom operation duration",
    ["operation_type"],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0]
)

# Custom gauge
custom_gauge = metrics.create_custom_gauge(
    "custom_resource_usage",
    "Custom resource usage",
    ["resource_type"]
)
```

### **Distributed Tracing with Jaeger**

```python
from telemetry import OpenTelemetryTracer

tracer = OpenTelemetryTracer(
    service_name="market-intel-brain",
    jaeger_endpoint="localhost:6831",
    sample_rate=0.1  # 10% sampling
)

# Spans automatically sent to Jaeger
with tracer.trace_async("distributed_operation") as span:
    # Your distributed operation here
    result = await call_external_service()
    return result
```

### **Data Collection and Aggregation**

```python
from telemetry import get_collector

collector = get_collector()
await collector.start()

# Add various events
collector.add_performance_event("api_call", 0.5, "provider1")
collector.add_error_event(exception, "provider1", "fetch")
collector.add_metric_event("cpu_usage", 75.5, {"host": "server1"})

# Get aggregated data
aggregated = collector.get_aggregated_data()
print(f"Performance stats: {aggregated}")
```

### **Prometheus Exporter**

```python
from telemetry import get_exporter, start_exporter

# Start exporter
exporter = get_exporter(port=8000)
await start_exporter()

# Metrics available at http://localhost:8000/metrics
# Grafana can scrape this endpoint
```

## ğŸ“Š **Metrics Reference**

### **Request Metrics**

| Metric Name | Type | Labels | Description |
|------------|------|--------|-------------|
| `adapter_requests_total` | Counter | provider, operation, status | Total adapter requests |
| `adapter_request_duration_seconds` | Histogram | provider, operation | Request duration distribution |
| `adapter_active_requests` | Gauge | provider | Currently active requests |
| `adapter_errors_total` | Counter | provider, operation, error_type | Total errors |
| `adapter_success_rate` | Gauge | provider | Success rate (0-1) |

### **Cache Metrics**

| Metric Name | Type | Labels | Description |
|------------|------|--------|-------------|
| `adapter_cache_hits_total` | Counter | provider, cache_type | Total cache hits |
| `adapter_cache_misses_total` | Counter | provider, cache_type | Total cache misses |

### **System Metrics**

| Metric Name | Type | Labels | Description |
|------------|------|--------|-------------|
| `adapter_data_volume_total` | Counter | provider, data_type | Total data volume |
| `adapter_rate_limited_total` | Counter | provider | Rate limit events |
| `adapter_connection_errors_total` | Counter | provider | Connection errors |
| `adapter_timeouts_total` | Counter | provider, operation | Timeout events |

## ğŸ” **Monitoring and Visualization**

### **Grafana Dashboard Setup**

1. **Add Prometheus Data Source**
   - URL: `http://localhost:8000/metrics`
   - Name: `Market Intel Brain`

2. **Create Dashboard Panels**
   - **Request Rate**: `rate(adapter_requests_total[5m])`
   - **Request Duration**: `histogram_quantile(0.95, rate(adapter_request_duration_seconds[5m]))`
   - **Success Rate**: `adapter_success_rate`
   - **Error Rate**: `rate(adapter_errors_total[5m])`
   - **Active Requests**: `adapter_active_requests`

3. **Alerting Rules**
   - High error rate: `rate(adapter_errors_total[5m]) > 0.1`
   - High latency: `histogram_quantile(0.95, rate(adapter_request_duration_seconds[5m])) > 2.0`
   - Low success rate: `adapter_success_rate < 0.95`

### **Jaeger Tracing Setup**

1. **Add Jaeger Data Source**
   - URL: `http://localhost:16686`
   - Name: `Jaeger`

2. **Create Tracing Dashboard**
   - Service Map: Visualize service dependencies
   - Trace Search: Query and analyze traces
   - Performance Metrics: Trace duration and error rates

## ğŸ§ª **Testing**

### **Run Examples**

```bash
python example_usage.py
```

### **Unit Tests**

```python
import pytest
from telemetry import get_tracer

@pytest.mark.asyncio
async def test_tracer():
    tracer = get_tracer()
    
    with tracer.trace_async("test_operation") as span:
        span.set_attribute("test", True)
        assert span is not None
```

## ğŸ”§ **Configuration**

### **Tracer Configuration**

```python
from telemetry import OpenTelemetryTracer

tracer = OpenTelemetryTracer(
    service_name="market-intel-brain",
    jaeger_endpoint="jaeger:6831",
    enable_tracing=True,
    sample_rate=0.1  # 10% sampling for production
)
```

### **Metrics Configuration**

```python
from telemetry import PrometheusMetrics, MetricConfig

config = MetricConfig(
    namespace="market_intel_brain",
    subsystem="adapters",
    push_gateway="prometheus-push:9091",
    push_interval=60
)

metrics = PrometheusMetrics(config)
```

### **Collector Configuration**

```python
from telemetry import TelemetryCollector, CollectorConfig

config = CollectorConfig(
    max_events=10000,
    batch_size=100,
    flush_interval=5.0,
    enable_persistence=True,
    persistence_file="telemetry_events.jsonl"
)

collector = TelemetryCollector(config)
```

## ğŸš¨ **Best Practices**

### **1. Sampling Strategy**

```python
# Production: Use sampling to reduce overhead
sample_rate = 0.01  # 1% sampling for high traffic

# Development: Use full sampling
sample_rate = 1.0    # 100% sampling for debugging
```

### **2. Metric Naming**

```python
# Use consistent naming conventions
# Format: {component}_{object}_{metric_type}_{unit}

# Examples:
# adapter_requests_total
# adapter_request_duration_seconds
# adapter_cache_hits_total
```

### **3. Label Strategy**

```python
# Use consistent labels
# Always include: provider, operation
# Optional: status, error_type, region, version

# Examples:
# labels(provider="finnhub", operation="fetch", status="success")
# labels(provider="yahoo", operation="sync", error_type="timeout")
```

### **4. Error Handling**

```python
# Always record context with errors
try:
    result = await risky_operation()
except Exception as e:
    # Record error with full context
    metrics.record_request_error(provider, "operation", type(e).__name__)
    tracer.add_span_event(span, "error", {"error": str(e), "stack": traceback.format_exc()})
    raise
```

## ğŸ“ˆ **Performance Considerations**

### **Overhead Analysis**

- **Tracing Overhead**: ~5-10ms per span with sampling
- **Metrics Overhead**: ~1-2ms per metric update
- **Memory Usage**: ~1MB per 10,000 events
- **Network Bandwidth**: ~1KB per 1000 metrics

### **Optimization Tips**

```python
# Use sampling for high-traffic scenarios
sample_rate = 0.01  # 1% sampling

# Batch metric updates
metrics.record_request_batch(requests)  # Instead of individual calls

# Use async processing
async def process_events():
    await collector.process_events()  # Background processing
```

## ğŸ”„ **Integration Examples**

### **Web Application Integration**

```python
from telemetry import telemetry_decorator

@app.telemetry_middleware
@app.get("/api/data/{symbol}")
async def get_data(symbol: str):
    # Automatically traced and measured
    data = await fetch_data(symbol)
    return {"symbol": symbol, "data": data}
```

### **Background Job Monitoring**

```python
from telemetry import get_metrics

async def background_job():
    metrics.record_request_start("background", "job1")
    
    try:
        result = await process_background_job()
        metrics.record_request_success("background", "job1", duration)
    except Exception as e:
        metrics.record_request_error("background", "job1", type(e).__name__)
    finally:
        metrics.record_request_end("background")
```

### **Multi-Service Tracing**

```python
# Service A
with tracer.trace_async("service_a_operation") as span:
    span.set_attribute("service", "service_a")
    
    # Call Service B with context propagation
    headers = tracer.inject_context()
    result = await call_service_b(headers)
    
    # Service B automatically receives the context
    # and continues the same trace
```

## ğŸ“š **Dependencies**

- **opentelemetry-api>=1.20.0** - OpenTelemetry API
- **opentelemetry-sdk>=1.20.0** - OpenTelemetry SDK
- **opentelemetry-exporter-jaeger>=1.20.0** - Jaeger exporter
- **prometheus-client>=0.16.0** - Prometheus client library
- **aiohttp>=3.8.0** - HTTP server for exporter

## ğŸ¤ **Contributing**

When contributing to the telemetry system:

1. **Test instrumentation thoroughly** with various scenarios
2. **Validate metric names** follow conventions
3. **Check performance impact** of new features
4. **Update documentation** for new components
5. **Test with real workloads** before deployment

## ğŸ“„ **License**

This telemetry system is part of the Market Intel Brain project.
